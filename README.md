# LLMs From Scratch

> Building and training Large Language Models from the ground up

## ğŸŒŸ Overview

A full-stack ChatGPT-like application built (almost) from scratch, featuring real-time conversation capabilities, multi-modal support, and a modern web interface. This project demonstrates the implementation of various components of a production-ready LLM application, from model training to deployment. 

## âœ¨ Features
- ğŸ¤– Multiple LLM Architecture Support
  - Gemma-3-1b-it
  - Qwen2.5-0.5B-Instruct
  - SmolVLM-256M-Instruct (Multi-modal)
- ğŸ’¬ Real-time Conversation
  - WebSocket-based streaming responses
  - Token-by-token generation
- ğŸ¨ Modern Web Interface (vibe coded)
  - React + TypeScript
  - Tailwind CSS for styling
- ğŸ–¼ï¸ Multi-modal Capabilities
  - Image upload and processing
  - Vision-language model integration
- ğŸ’¾ Persistent Storage
  - SQLite database
  - Message and conversation history
- ğŸ³ Containerization
  - Docker support for both frontend and backend
  - Easy deployment and scaling

## ğŸ› ï¸ Technical Stack
### Frontend
- React 18
- TypeScript
- Tailwind CSS
- WebSocket for real-time communication

### Backend
- FastAPI
- SQLAlchemy with SQLite
- PyTorch
- Transformers
- WebSockets
- Docker

## ğŸš€ Getting Started

### Prerequisites
- Docker and Docker Compose
- Python 3.8+
- Node.js 16+

### Installation
1. Clone the repository
```bash
git clone https://github.com/snnclsr/chatgpt-from-scratch.git
cd chatgpt-from-scratch
```

2. Start the backend
```bash
cd backend
docker build -t chatgpt-backend .
docker run -p 8000:8000 chatgpt-backend
```

3. Start the frontend
```bash
cd frontend
docker build -t chatgpt-frontend .
docker run -p 3000:3000 chatgpt-frontend
```

## ğŸ“š Model Training
This project builds upon Sebastian Raschka's "Build a Large Language Model (From Scratch)" book, implementing:
- Custom GPT model architecture
- Instruction tuning using the Alpaca dataset
- Model conversion techniques (GPT-2 to Llama variants)
- Preference tuning capabilities

## ğŸ—ï¸ Project Structure

```bash
â”œâ”€â”€ backend
â”‚Â Â  â”œâ”€â”€ ml
â”‚Â Â  â”‚Â Â  â””â”€â”€ providers
â”‚Â Â  â”œâ”€â”€ models
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ SmolVLM-256M-Instruct
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ onnx
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ gemma-3-1b-it
â”‚Â Â  â”‚Â Â  â””â”€â”€ gemma-3-4b-it
â”‚Â Â  â”œâ”€â”€ my_ml
â”‚Â Â  â”œâ”€â”€ repositories
â”‚Â Â  â”œâ”€â”€ routes
â”‚Â Â  â”‚Â Â  â””â”€â”€ websockets
â”‚Â Â  â”œâ”€â”€ services
â”‚Â Â  â”œâ”€â”€ uploads
â”‚Â Â  â””â”€â”€ utils
â”œâ”€â”€ frontend
â”‚Â Â  â”œâ”€â”€ public
â”‚Â Â  â””â”€â”€ src
â”‚Â Â      â”œâ”€â”€ components
â”‚Â Â      â””â”€â”€ services
â”œâ”€â”€ modelling
```

## ğŸ™ Acknowledgments
- Sebastian Raschka's "Build a Large Language Model (From Scratch)" book
- Alpaca dataset (CC BY-NC 4.0)
- Open-source model providers (Gemma, Qwen, SmolVLM)

